# Example environment variables for Gemini CLI
# Copy this file to .env and configure the appropriate values for your setup

# === Gemini API Key ===
# Get your API key from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your_gemini_api_key_here

# === OpenRouter ===
# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional: Specify a default OpenRouter model
# See available models at: https://openrouter.ai/models
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# === Custom OpenAI-Compatible Provider ===
# For custom OpenAI-compatible endpoints (Ollama, LM Studio, etc.)
# Configure in ~/.gemini/settings.json instead:
# {
#   "selectedAuthType": "custom-openai",
#   "customOpenAIConfig": {
#     "apiKey": "your-api-key",
#     "baseUrl": "http://localhost:11434/v1",
#     "model": "llama3.1:70b"
#   }
# }

# === Vertex AI ===
# For Vertex AI authentication (requires gcloud setup)
# GOOGLE_CLOUD_PROJECT=your_project_id
# GOOGLE_CLOUD_LOCATION=us-central1
# GOOGLE_GENAI_USE_VERTEXAI=true

# Or for Vertex AI express mode:
# GOOGLE_API_KEY=your_vertex_api_key
# GOOGLE_GENAI_USE_VERTEXAI=true

# === Google OAuth (Code Assist) ===
# For Google Workspace or other special cases:
# GOOGLE_CLOUD_PROJECT=your_project_id

# === General Settings ===
# Optional: Specify a different model (when using Gemini API)
# GEMINI_MODEL=gemini-2.5-pro

# Optional: Proxy settings
# HTTPS_PROXY=http://proxy.example.com:8080
# HTTP_PROXY=http://proxy.example.com:8080
